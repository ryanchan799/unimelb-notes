COMP30023 - Computer Systems Tutorial Week 2 Attempt

1. Advantages:
	- Allows for different platforms to communicate over a common protocol
	- Easy to implement a protocol that already exists
   Disadvantages:
   	- Vulnerabilities are hard to fix
   	- Functionality limited

2.	Nothing is changed, as services to upper layers are not affected - only the algorithm within layer k

3. 	Only layers k+1 are affected as the services provided are no longer the same

4.	n*h/M+(n*h)

5. Mail: POP3
	Remote Login: Telnet/SSH
	Streaming: RTP
	Web: HTTP
	File Transfer: FTP

6. A cookie can be stored on a user's device which will provided non-deterministic session data. This e-commerce site can then cache this data depending on what websites are accessed and build a purchase record.

7. a) cis.unimelb.edu.au/people/index.html
	b) 1.1
	c) Persistent
	d) ? Can't get it
	e) Chrome. Different browsers support different features in frameworks such as CSS etc.

8. a) False, assuming the user is using http1.0 then  
	b) True - same domain?
	c) False - can't carry two
	d) False - Last requested?
	e) False

# Tutorial 2

DNS/MAil

1. All users now have to remember IP addresses of all the sites used. One can no longer enter an address to access a website.

2. The Resource Record value would contain different values depending on if it was the web server or mail server. The Mail server would contain an "MX" mail server type.

3. This is not an issue as a DNS packet is typically very small and can be transmitted in one UDP dataframe. Because of this, if a packet is lost, the client can simply just request another DNS packet. UDP is overall less resource intensive than TCP, and therefore is used for DNS servers as they are subject to huge bandwidth costs which would be compounded exponentially by TCP.

4. This can be done by simply querying the caches of the DNS throughout the day to see which sites are accessed consistently. This works as the cache contains the last most accessed websites to decrease query times. This cache therefore can be checked throughout the day to see what sites are stored in the cache throughout the day.

5.  Yes. If the query speed is considerably faster, (100ms decrease) then it is likely stored on the DNS cache and therefore was already accessed previously (and therefore stored on the DNS cache) by a computer.

6. It is part of the user agent?

7. This means that if a business email is forwarded to John's personal address a vacation response will be sent back to John's work machine. This will result in this vacation message being sent from John's machine back to his personal computer, resulting in an infinite loop of vacation emails being sent back and forth.

8. unimelb lel

# Tutorial 3

1. 
Type	TCP	|	UDP
a)		Yes	|	No
b)		No  |	No
c)		Yes |	No
d)		No 	| 	No
e) 		No? |	Yes
f) 		Yes |	No

2.	UDP exists in order to allow for port forwarding and sending of large files. Sending just raw IP packets would not allow for a destination/source port to be defined, and it would not fragment the data for transfer, resulting in the whole file having to be transferred at one attempt with no packet loss.

3. A process ID is *dynamic* and may change over the course of a operating system's uptime. This means that messages may be incorrectly delivered to different processes.
	- IDs are OS-specific
	- Single process may establish multiple channels of communications
	- Single process ID cannot be used to distinguish between these channels
	- Well-known ports vs well-known process IDs are impossible

 4. You could use TCP instead. TCP is reliable, as each message has a confirmation response and therefore has no lost packets.

 5. No? Single connections are only between two ports.

 6. From the number of pages and offset you can calculate the page sizes. Page sizes that are not powers of two are difficult to calculate and are therefore typically not assigned by operating systems. (as if a page is a power of 2 a bitwise operation can simply be done)

 7. a) PTE = 4 bytes so 4kb to map 512kb and 32mb to map 4gb

 8. 4096 / 12 = 340 pages?

 9. A situation where the demand of physical memory of the processes in memory consistently exceeds the available physical memory. This results in a *constant exchange* of pages between memory and disk; which slows down actual application processing tasks.

 # Week 4

 1. A race condition is where the final result depends on *when the processes are run*. This may affect the correctness of the final result. A deadlock is where each threads are stuck waiting for each other resulting in an infinite loop.

 2. User threads are cheap to create and do not require kernel privileges. The disadvantage is that when a system call is made by a user thread it blocks the process until the system call is complete.

 3. Kernel level threads? Since a user thread would block the process during a system call, a kernel level thread must be used otherwise multiple clients would not be able to recieve data.

 A worker thread will block when it has to read a web page from the disk. If user level threads are being used,this action will block the entire process, destroying the value of multithreading. Thus it is essential that kernel threads are used to permit some threads to block without affecting the others.

 4. This is to prevent data leakage done for security reasons. This stops the interrupted process from accessing privileged memory space. 

 Because the kernel may process privileged information. In the course of this processing, the privileged information may end up in local variables stored on the stack. We don't want user programs to access this information, so we have to make sure that user programs cannot access the stack frames used by the kernel. The simplest way of ensuring this is making the kernel use a separate stack that user prorams do not have access to. (In threory we could make the kernel use the user stack but require the kernel to zero out every byte of the user stack that the kernel has used, but there is no easy way to find out which bytes of the user stack have been used).

 Another reason is that the user stack may be near its maximum size and we don't want the kernel to handle any errors that result from this.

 5. 65515 for TCP segment + header inside the IP packet. The header is 20 bytes so this leaves 65495

 The entire TCP segment must fit in the 65515 byte payload field of an IP packet. Since the TCP header is a minimum of 20 bytes, only 65495 bytes are left for TCP data.

6. 64-bit -> 75 Tbps

75 Tbps -> 75 * 10^9 / 64

The size of the sequence space is 2<super>64</super> bytes, which is about 2\*10<super>19</super> bytes. A 75 transmitter uses up sequence space at a rate of 9.375\*10<super>12</super> sequence numbers per second. It takes 2 million seconds to wrap around. Since there are 86400 seconds in a day, it will take over 3 weeks to wrap around, even at 75 Tbps. A maximum packet lifetime of less than 3 weeks will prevent the problem. In short, going to 64 bits is likely to work for a while.

7. a) SYN flooding
b) When the client responds in the 3-way handshake, they must respond with this cookie ISN. The server *will not* allocate resources initially like it usually does until it receives this cookie.
c) This is done by hashing the sequence number such that the ACK is equivalent to the SYN.
d) Simply hash the ACKs according to the time. If it doesn't match, then don't accept the connection.

a) In this attach, the attachers send a large number of TCP SYN segments, without completing the third handshake step. With this deluge of SYN segments, the servers connection resources become exhausted as they are allocated but never used for half open connections; legitimate clients are then denied service.

b) The server creates an initial TCP sequence number that is a complicated hash function of source and destination IP addresses and port numbers of the SYN segment *as well as a secret number only known to the server*. This carefully crafted initial sequence number is the so-called cookie. The server then sends the client a SYNACK packet with this special sequence number. Note that the server does not remember the cookie or any other state information corresponding to the SYN.

c) For a legitimate ACK, the value in the acknowledgemnet field is equal to the initial sequence number in the SYNACK plus one. The server can then run the same hash function using the source/destination IP address and port numbers in the SYNACK and the secret number. If the result of the function + 1 is the same as the acknowledgement value in the client's SYNACK, the server concludes that the ACK corresponds to an earlier SYN segment and is hence valid. The server the creates a fully open 

Tutorial 6
1. a) 20 bytes?
b) The acknowledgement number would still be 90

2.
a) 127+80, 302, 80  (this assumes the second segment arrives after the first segment has been ACKed)
b) 207, 80, 302
c) 126+1 (waiting for more bytes)

3. Congestion control focuses on managing traffic congestion in a network by limiting transfer rates. Flow control focuses on managing congestion by directing traffic to alternative routes. In congestion control, the TCP sliding winder is used, whereas in flow control, a routing table is used.

A *flow control service* is used to *elimate the possibility of the sender overflowing the receiver's buffer*. Flow control is thus a speed-matching service - matching the rate at which the sender is sending against the rate at which the receiving application is reading. A *TCP sender can also be throttled due to congestion within the IP network*; this form of sender control is referred to as *congestion control*.

*Flow Control*: Sliding Window Protocols
*Congestion Control*: Slow Start Algorithm

4. This can occur when the window size is zero and the sender does not know when to update their window size. This will result in no segments being sent. This is solved by a persist timer, where after a certain amount of time a ZeroWindowUpdateProbe is sent, which updates the receiver's window and the sender's window when a ZeroWindowUpdateProbeACK is sent back.

Consider the situation where Host A is sending a large file to Host B. Suppose the receive buffer on Host B becomes full, and after advertising that its receiver window is zero Host B has nothing to send to Host A. Therefore, even when the application process at B empties the buffer, TCP does not send new segments with new receiver values to Host A; indeed, TCP sends a segment to Host A only if it has data to send or if it has a acknowledgement to send. Therefore, Host A is never informed that some space has opened up in Host B's receive buffer. Host A is blocked and can transmit no more data.
To solve this probloem, the TCP specification requires Host A to continue to send segments with one data byte when B's receive window is zero (a window probe). These segments will be ackowledged by the receiver. Eventually the buffer will begin to empty and the acknowledgements will contain a nonzero receive window value.

5. The slow start algorithm operates by limited rates based on the rate of packet loss within the network. When a packet is lost, the slow start algorithm halves it's maximum transfer threshold and starts from zero again. Another method that could be used is simply checking resource usage in the server.

TCP segment loss is taken as an indication of network congestion and TCP decreases its window size accordingly. Other methods may include using increasing round-trip delay values as indicators of increased network congestion. Network assist congestion control mechanisms may use Explicit Congestion Notification, where a router marks/updates a field in a packet flowing from sender to receiver to indicate congestion. Upon receipt of a market packet, the receiver then notifies the sender of the congestion indication.

6. a) False
b) True
c) False, it'll be acknowledgement number+1
d) True

# Tutorial 7

1. Yes. One for each interface i.e. each port.

2. No. In the case that a node in the network goes down an alternative path will have to be considered.

3. 
240/2 = 0
120/2 = 0
60/2  = 0
30/2  = 0
15/2  = 1
7/2   = 1
3/2   = 1
1/2   = 1

= 4 bits + 8 bits = 12 bits
= 12^2 hosts

4. 57.6.00000|000.00000000

96/2 = 0
48/2 = 0
24/2 = 0
12/2 = 0
6/2 = 0
3/2 = 1
1/2 = 1
    = 0
96:
57.6.0110

128 64 32 16 8 4 2 1
 0   1  1  0 1 0 0 0

104:
57.6.01101

112:
57.6.01110

120:
57.6.01111

Yes, the can be aggregated, no conflicts.

5. 192.48.0.0

A = 4000 approx 2^12 hosts
B = 2000 approx 2^11 hosts
C = 8000 approx 2^13 hosts

12 bits for the host
192.48.0000|0000.00000000 -> 192.48.0.0/20
192.48.0000|1111.11111111 -> 192.48.15.255/20

11 bits for the host
192.48.00010|000.00000000 -> 192.48.16.0/21
192.48.00010|111.11111111 -> 192.48.23.255/21

13 bits for the host
192.48.001|0000.00000000 -> 192.48.32.0/19
192.48.001|1111.11111111 -> 192.48.63.255/19

6.

192.53.0010100|0.00000000

135.46.001110|00.00000000


135.46.001111|00.00000000

a) Interface 1
b) Interface 0
c) Router 2
d) Router 1
e) Router 2 lol

7.
In order to cause a DDoS attack and deny any service to normal users of that IP address. 

8. In case the IP packet ends up in an infinite routing loop it can be "killed" to prevent itself from endlessly hopping from router to router and causing congestion.

9. a long fucking time

# Tutorial 8

1. 101.101.128.0/17

32-17 = 15
101.101.1|0000000.00000000

Create using truth table like thing
101.101.100|00000.00000000 -> 128.0/19
101.101.101|00000.00000000 -> 160.0/19
101.101.110|00000.00000000 -> 192.0/19
101.101.111|00000.00000000 -> 224.0/19

2.
a) Happens at the link layer
b) 4 fragments are generated
c) DF (don't fragment), FO (fragmentation offset - size of the last fragment + 8) and MF (more fragments)
d) 500

3. a) No, we'd have run out of IP addresses by now
b) Increment up for the last bit lol
c) NAT - Translate a public IP into a private IP using the port number as a reference
d) Ports may not match up after going through the NAT

4.
a) A, B and C are different hosts
b) Depends on the agreements between the different ISP providers but generally not? Must be end-to-enda

128.119.40.01|00|0000
128.119.40.01|01|0000 -> 96
128.119.40.01|10|0000 -> 128
128.119.40.01|11|0000 -> 160



223.1.17.|00000000
Subnet 1 = 223.1.17.00|000000\26
Subnet 2 = 223.1.17.1|00000000\25
Subnet 3 = 223.1.17.0100|0000\28

223.1.17.0/26
223.1.17.128/25
223.1.17.64/28